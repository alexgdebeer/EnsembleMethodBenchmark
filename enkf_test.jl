using Distributions
using LinearAlgebra
using Statistics

using LaTeXStrings
import PyPlot
import Seaborn

include("lokta_volterra_model.jl")
include("sim_intensive_inference/sim_intensive_inference.jl")

PyPlot.rc("text", usetex=true)
PyPlot.rc("font", family="serif")

# Define plotting font sizes
const TITLE_SIZE = 20
const LABEL_SIZE = 16
const SMALL_SIZE = 8


function run_enkf(
    f::Function,
    H::Function,
    π_u::SimIntensiveInference.AbstractPrior,
    ts::Vector,
    ys::Matrix,
    t_1::Real,
    σ_ϵ::Real,
    N_e::Int
)

    # Generate an initial sample of states from the prior
    us_e = SimIntensiveInference.sample(π_u, n=N_e)

    # Define a vector that offsets the times by 1
    ts_0 = [0.0, ts[1:(end-1)]...]

    us_e_combined = reduce(vcat, us_e)
    us_e = reduce(hcat, us_e)

    for (t_0, t_1, y) ∈ zip(ts_0, ts, eachcol(ys))

        # Run each ensemble member forward in time 
        us_e = [f(LVModel.θS_T; y_0=u, t_0=t_0, t_1=t_1)[:, 2:end] for u ∈ eachcol(us_e)]
        us_e_combined = hcat(us_e_combined, reduce(vcat, us_e))

        # Extract the forecast states and generate the predictions 
        us_ef = reduce(hcat, [u[:, end] for u ∈ us_e])
        ys_ef = H(us_ef, LVModel.θS_T)

        # Generate a set of perturbed data vectors 
        Γ_ϵϵ = σ_ϵ^2 * Matrix(I, length(y), length(y))
        ys_p = reduce(hcat, [rand(MvNormal(y, Γ_ϵϵ)) for _ ∈ 1:N_e])

        # Compute the covariance of the predicted data 
        U_c = us_ef * (I - ones(N_e, N_e)/N_e)
        Y_c = ys_ef * (I - ones(N_e, N_e)/N_e)
        Γ_uy_e = 1/(N_e-1)*U_c*Y_c'
        Γ_yy_e = 1/(N_e-1)*Y_c*Y_c'

        K = Γ_uy_e * inv(Γ_yy_e + Γ_ϵϵ)
        us_e = us_ef + K*(ys_p-ys_ef)

    end

    # Run each ensemble member to the final timestep if necessary
    if ts[end] < t_1
        us_e = [f(LVModel.θS_T; y_0=u, t_0=ts[end])[:, 2:end] for u ∈ eachcol(us_e)]
        us_e_combined = hcat(us_e_combined, reduce(vcat, us_e))
    end

    return us_e_combined

end


# Define parameters of the prior for the model state 
const μ_u = [0.75, 0.75]
const σ_u = 0.5
const Σ_u = σ_u^2 * Matrix(I, 2, 2)
const π_u = SimIntensiveInference.GaussianPrior(μ_u, Σ_u)

# Define ensemble size 
const N_e = 100


us_e = run_enkf(
    LVModel.f, 
    LVModel.H, 
    π_u, 
    LVModel.TS_O, 
    LVModel.YS_O,
    LVModel.T_1,
    LVModel.σ_ϵ, 
    N_e
)

fig, ax = PyPlot.subplots(1, 2, figsize=(7, 4))

# Extract the observations generated by the ensemble
y1s = us_e[mod.(1:2N_e,2).==1, :]
y2s = us_e[mod.(1:2N_e,2).==0, :]

# Compute some quantiles of the ensemble
y1_qs = mapslices(c -> quantile(c, [0.025, 0.975]), y1s, dims=1)
y2_qs = mapslices(c -> quantile(c, [0.025, 0.975]), y2s, dims=1)

# Plot the true model states
ax[1].plot(LVModel.TS, LVModel.YS_T[1, :], c="k", ls="--", zorder=4)
ax[2].plot(LVModel.TS, LVModel.YS_T[2, :], c="k", ls="--", zorder=4)

# Plot the observations
ax[1].scatter(LVModel.TS_O, LVModel.YS_O[1, :], color="k", marker="x", zorder=4)
ax[2].scatter(LVModel.TS_O, LVModel.YS_O[2, :], color="k", marker="x", zorder=4)

# Plot the ensemble
ax[1].plot(LVModel.TS, y1s', color="gray", alpha=0.5, zorder=2)
ax[2].plot(LVModel.TS, y2s', color="gray", alpha=0.5, zorder=2)

# Plot the quantiles 
ax[1].plot(LVModel.TS, y1_qs', color="red", zorder=3)
ax[2].plot(LVModel.TS, y2_qs', color="red", zorder=3)

ax[1].set_ylim((-1, 4))
ax[2].set_ylim((-1, 4))

ax[1].set_xlabel(L"t", fontsize=LABEL_SIZE)
ax[2].set_xlabel(L"t", fontsize=LABEL_SIZE)
ax[1].set_ylabel(L"y_{1}(t)", fontsize=LABEL_SIZE)
ax[2].set_ylabel(L"y_{2}(t)", fontsize=LABEL_SIZE)

fig.suptitle("Ensemble Kalman Filter", fontsize=TITLE_SIZE)

PyPlot.tight_layout()
PyPlot.savefig("test.pdf")